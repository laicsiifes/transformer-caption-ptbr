##########################################################
################### 1. GENERAL CONFIGS ###################
##########################################################

config:
  # multimodal large language model
  # model_name: "llama3-vision-unsloth"
  # model_name: "phi3-vision"
  # model_name: "paligemma"
  model_name: "vitucano-1b" # In progress...
  # model_name: "vitucano-2b" # In progress...

  # dataset for training and evaluation
  dataset: "flickr30k_pt"
  # dataset: "flickr30k_pt_human_generated"
  # dataset: "pracegover_63k"

  # enable to upload model and processor to hub
  push_to_hub: True

  # dataset from hub
  dataset_from_hub: True

  # training/evaluation interval type ("epoch", "steps", "no") and
  # monitoring steps for eval_steps and logging_steps
  interval_strategy: "steps" # "epoch" # 
  monitoring_steps: 200

  # flag to eval.py, if True it evaluate using the model
  # else it evaluate using the predictions previously saved after training
  evaluate_from_model: False

  # flag to turn off the computer after the training
  turn_off_computer: False

  # True to save with suffix -ft, False to save without. Don't change.
  use_adapters: True


##########################################################
################# 2. GENERATION CONFIGS ##################
##########################################################

generate_args:
  # temperature": 0.0
  # do_sample": False
  # num_beams: 5 # default 5
  # no_repeat_ngram_size: 0 # default 0
  # early_stopping: False # default False
  # max_length: 25 # default 20
  max_new_tokens: "auto" # automatically set by dataset max_length


##########################################################
################## 3. TRAINING CONFIGS ###################
##########################################################

callbacks:
  early_stopping:
    patience: 3 # default 1
    threshold: 0.0 # default 0.0


##########################################################
#################### 4. MODELS CONFIGS ###################
##########################################################

mllm:
  # LlaMa 3.2 Vision Unsloth
  llama3-vision-unsloth:
    id: "unsloth/Llama-3.2-11B-Vision-Instruct"
    batch_size: 1
    use_flash_attention: False
    question: "Escreva uma descrição em português do Brasil para a imagem com no máximo {max_length} palavras."
    # QLoRa configuration
    qlora_args:
      use_bnb: True # BitsAndBytes quantization
      use_lora: True # LoRA
      lora_rank: 8 # by model authors
      alpha_to_rank_ratio: 4 # by model authors
      dropout: 0.05 # by model authors
      lora_all_linear: False # Not stable, keep False
      linear_modules: [ # by model authors
        'q_proj',
        # 'o_proj',
        # 'k_proj',
        'v_proj',
        # 'gate_proj',
        # 'up_proj',
        # 'down_proj'
      ]
    # Training arguments
    training_args:
      num_train_epochs: 1
      do_eval: False
      gradient_accumulation_steps: 4 # by model authors
      gradient_checkpointing: True
      gradient_checkpointing_kwargs:
        use_reentrant: False  # NOTE important for LoRA
      save_total_limit: 1
      # warmup_steps: 2
      # max_grad_norm: 1
      # warmup_ratio: 0.1
      weight_decay: 0.0  # by model authors
      fp16: True # by model authors
      optim: "paged_adamw_8bit" # by QLoRA, to avoid memory spikes when evaluating large captions
      learning_rate: 1.0e-4 # by model authors
      load_best_model_at_end: False
      metric_for_best_model: "rougeL" # used in previous work as objective function, nice distribution
      greater_is_better: False
      push_to_hub: False
      remove_unused_columns: False
      dataloader_pin_memory: False
      predict_with_generate: False # Only with Seq2SeqTrainer and Seq2SeqArguments
      # label_names: ['labels']
      # dataset_kwargs:
      #   skip_prepare_dataset: True

  # LlaMa 3.2 Vision
  llama3-vision:
    id: "meta-llama/Llama-3.2-11B-Vision-Instruct"
    batch_size: 1
    use_flash_attention: False
    question: "Escreva uma descrição em português do Brasil para a imagem com no máximo {max_length} palavras."
    # QLoRa configuration
    qlora_args:
      use_bnb: True # BitsAndBytes quantization
      use_lora: True # LoRA
      lora_rank: 8 # by model authors
      alpha_to_rank_ratio: 4 # by model authors
      dropout: 0.05 # by model authors
      lora_all_linear: False # Not stable, keep False
      linear_modules: [ # by model authors
        'q_proj',
        # 'o_proj',
        # 'k_proj',
        'v_proj',
        # 'gate_proj',
        # 'up_proj',
        # 'down_proj'
      ]
    # Training arguments
    training_args:
      num_train_epochs: 1
      do_eval: False
      gradient_accumulation_steps: 4 # by model authors
      gradient_checkpointing: True
      gradient_checkpointing_kwargs:
        use_reentrant: False  # NOTE important for LoRA
      save_total_limit: 1
      # warmup_steps: 2
      # max_grad_norm: 1
      # warmup_ratio: 0.1
      weight_decay: 0.0  # by model authors
      fp16: True # by model authors
      optim: "paged_adamw_8bit" # by QLoRA, to avoid memory spikes when evaluating large captions
      learning_rate: 1.0e-4 # by model authors
      load_best_model_at_end: False
      metric_for_best_model: "rougeL" # used in previous work as objective function, nice distribution
      greater_is_better: False
      push_to_hub: False
      remove_unused_columns: False
      dataloader_pin_memory: False
      predict_with_generate: False # Only with Seq2SeqTrainer and Seq2SeqArguments
      # label_names: ['labels']
      # dataset_kwargs:
      #   skip_prepare_dataset: True

  # Phi-3 Vision
  phi3-vision:
    id: "microsoft/Phi-3-vision-128k-instruct"
    batch_size: 1
    use_flash_attention: False
    question: "Escreva uma descrição em português do Brasil para a imagem com no máximo {max_length} palavras."
    # QLoRa configuration
    qlora_args:
      use_bnb: True # BitsAndBytes quantization
      use_lora: True # LoRA
      lora_rank: 64 # by model authors
      alpha_to_rank_ratio: 2.0 # by model authors
      dropout: 0.0 # by model authors
      lora_all_linear: False # Not stable, keep False
      linear_modules: [ # by model authors
        # Phi language modules
        'qkv_proj', # attention
        'o_proj',
        'down_proj', # MLP
        'gate_up_proj',
        'lm_head',
        # CLIP modules
        'q_proj', # attention
        'k_proj',
        'v_proj',
        'out_proj',
        'fc1', # MLP
        'fc2',
        # image projection
        'img_projection.0',
        'img_projection.2',
      ]
    # Training arguments
    training_args:
      num_train_epochs: 1
      do_eval: False
      gradient_accumulation_steps: 4 # by model authors (batch_size // num_gpus, in our case 1/1)
      gradient_checkpointing: True # by model authors
      gradient_checkpointing_kwargs: # by model authors
        use_reentrant: False  # NOTE important for LoRA
      save_total_limit: 1
      warmup_steps: 50 # by model authors
      max_grad_norm: 1 # by model authors
      # warmup_ratio: 0.1
      weight_decay: 0.1 # by model authors
      bf16: True # by model authors
      adam_beta1: 0.9 # by model authors
      adam_beta2: 0.95 # by model authors
      adam_epsilon: 1.0e-7 # by model authors
      optim: "paged_adamw_8bit" # by QLoRA, to avoid memory spikes when evaluating large captions
      learning_rate: 4.0e-5 # by model authors
      load_best_model_at_end: False
      metric_for_best_model: "rougeL" # used in previous work as objective function, nice distribution
      greater_is_better: False
      push_to_hub: False
      remove_unused_columns: False
      dataloader_pin_memory: False
      predict_with_generate: False # Only with Seq2SeqTrainer and Seq2SeqArguments
      # label_names: ['labels']
      # dataset_kwargs:
      #   skip_prepare_dataset: True

  # PaliGemma
  paligemma:
    id: "google/paligemma-3b-pt-224"
    batch_size: 1
    use_flash_attention: False
    question: "caption pt\n" # by model authors
    # QLoRa configuration
    qlora_args:
      use_bnb: True # BitsAndBytes quantization
      use_lora: True # LoRA
      lora_rank: 8 # by model authors
      alpha_to_rank_ratio: 2.0
      dropout: 0.0
      lora_all_linear: False # Not stable, keep False
      linear_modules: [
        'q_proj',
        'o_proj',
        'k_proj',
        'v_proj',
        'gate_proj',
        'up_proj',
        'down_proj'
      ]
    # Training arguments
    training_args:
      num_train_epochs: 1
      do_eval: False
      gradient_accumulation_steps: 4
      gradient_checkpointing: True
      gradient_checkpointing_kwargs:
        use_reentrant: False  # NOTE important for LoRA
      save_total_limit: 1
      # warmup_steps: 2
      # max_grad_norm: 1
      # warmup_ratio: 0.1
      weight_decay: 1.0e-6 # by model authors
      bf16: True
      # adam_beta2: 0.999
      optim: 'paged_adamw_8bit' # by QLoRA, to avoid memory spikes when evaluating large captions
      learning_rate: 1.0e-5 # by model authors
      load_best_model_at_end: False
      metric_for_best_model: "rougeL"  # used in previous work as objective function, nice distribution
      greater_is_better: True
      push_to_hub: False
      remove_unused_columns: False
      dataloader_pin_memory: False
      predict_with_generate: False # Only with Seq2SeqTrainer and Seq2SeqArguments
      # label_names: ['labels']
      # dataset_kwargs:
      #   skip_prepare_dataset: True

  # ViTucano 1B
  vitucano-1b:
    id: "TucanoBR/ViTucano-1b5-v1"
    batch_size: 1
    use_flash_attention: False
    question: "Escreva uma descrição em português do Brasil para a imagem com no máximo {max_length} palavras."
    # QLoRa configuration
    qlora_args:
      use_bnb: False # BitsAndBytes quantization
      use_lora: False # LoRA
      lora_rank: 8 # by model authors
      alpha_to_rank_ratio: 2.0
      dropout: 0.0
      lora_all_linear: False # Not stable, keep False
      linear_modules: [
        'q_proj',
        'o_proj',
        'k_proj',
        'v_proj',
        'gate_proj',
        'up_proj',
        'down_proj'
      ]
    # Training arguments
    training_args:
      num_train_epochs: 1
      do_eval: False
      gradient_accumulation_steps: 4
      gradient_checkpointing: True
      gradient_checkpointing_kwargs:
        use_reentrant: False  # NOTE important for LoRA
      save_total_limit: 1
      # warmup_steps: 2
      # max_grad_norm: 1
      # warmup_ratio: 0.1
      weight_decay: 1.0e-6 # by model authors
      bf16: True
      # adam_beta2: 0.999
      optim: 'paged_adamw_8bit' # by QLoRA, to avoid memory spikes when evaluating large captions
      learning_rate: 1.0e-5 # by model authors
      load_best_model_at_end: False
      metric_for_best_model: "rougeL"  # used in previous work as objective function, nice distribution
      greater_is_better: True
      push_to_hub: False
      remove_unused_columns: False
      dataloader_pin_memory: False
      predict_with_generate: True # Only with Seq2SeqTrainer and Seq2SeqArguments
      # label_names: ['labels']
      # dataset_kwargs:
      #   skip_prepare_dataset: True

  # ViTucano 2B
  vitucano-2b:
    id: "TucanoBR/ViTucano-2b8-v1"
    batch_size: 1
    use_flash_attention: False
    question: "Escreva uma descrição em português do Brasil para a imagem com no máximo {max_length} palavras."
    # QLoRa configuration
    qlora_args:
      use_bnb: False # BitsAndBytes quantization
      use_lora: False # LoRA
      lora_rank: 8 # by model authors
      alpha_to_rank_ratio: 2.0
      dropout: 0.0
      lora_all_linear: False # Not stable, keep False
      linear_modules: [
        'q_proj',
        'o_proj',
        'k_proj',
        'v_proj',
        'gate_proj',
        'up_proj',
        'down_proj'
      ]
    # Training arguments
    training_args:
      num_train_epochs: 1
      do_eval: False
      gradient_accumulation_steps: 4
      gradient_checkpointing: True
      gradient_checkpointing_kwargs:
        use_reentrant: False  # NOTE important for LoRA
      save_total_limit: 1
      # warmup_steps: 2
      # max_grad_norm: 1
      # warmup_ratio: 0.1
      weight_decay: 1.0e-6 # by model authors
      bf16: True
      # adam_beta2: 0.999
      optim: 'paged_adamw_8bit' # by QLoRA, to avoid memory spikes when evaluating large captions
      learning_rate: 1.0e-5 # by model authors
      load_best_model_at_end: False
      metric_for_best_model: "rougeL"  # used in previous work as objective function, nice distribution
      greater_is_better: True
      push_to_hub: False
      remove_unused_columns: False
      dataloader_pin_memory: False
      predict_with_generate: True # Only with Seq2SeqTrainer and Seq2SeqArguments
      # label_names: ['labels']
      # dataset_kwargs:
      #   skip_prepare_dataset: True


##########################################################
############## 5. IMAGE CAPTIONING DATASETS ##############
##########################################################

dataset:
  # Flickr30K randomly sampled with 5k
  flickr30k_pt:
    id: "laicsiifes/flickr30k-pt-br-5k"
    max_length: 25
    image_column: "image"
    text_column: "caption"
    text_per_image: 5

  # Flickr30K randomly sampled with 5k with the human generated captions of FM30K
  flickr30k_pt_human_generated:
    id: "laicsiifes/flickr30k-pt-br-5k-human-generated"
    max_length: 25
    image_column: "image"
    text_column: "caption"
    text_per_image: 5

  # PraCegoVer randomly sampled with 5k
  pracegover_63k:
    id: "laicsiifes/pracegover63k-5k"
    max_length: 70
    image_column: "image"
    text_column: "text"
    text_per_image: 1
